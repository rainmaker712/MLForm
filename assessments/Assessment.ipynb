{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1123fe11-ac37-4070-9559-202a36d910e4",
   "metadata": {
    "id": "1123fe11-ac37-4070-9559-202a36d910e4"
   },
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d208e-ff07-44f6-b628-b82315c10ba4",
   "metadata": {
    "id": "f34d208e-ff07-44f6-b628-b82315c10ba4"
   },
   "source": [
    "# 최종 평가\n",
    "\n",
    "## 개요\n",
    "\n",
    "이번 교육 과정 평가의 목표는 거대 모델을 구축하고 실행할 수 있는 능력을 평가하는 것입니다. 기존 코드를 DeepSpeed로 이식하고 액티베이션 체크 포인팅, 혼합 정밀도 훈련, ZeRo 리던던시 옵티마이저 등 다양한 DeepSpeed 기능을 사용할 수 있도록 일련의 구성 파일을 활용해 봅니다.\n",
    "\n",
    "태스크를 포함할 수 있도록 단순화된 코드 베이스인 [minGPT](https://github.com/karpathy/minGPT) 를 의도적으로 선택했습니다. 본 코드는 최대 성능을 제공하지는 않지만 대표적이며 비교적 짧은 시간 내에 코딩 평가를 완료할 수 있도록 하는 Transformer의 최소 구현 형태입니다.\n",
    "\n",
    "이 과제에서 우리는 또 다른 모델인 비전 트랜스포머 모델(ViT)에 대해서도 살펴볼 것입니다. 평가 과제를 진행하기 전에 [코드 예제](minGPT/minGPT/play_image.ipynb)를 미리 검토하십시오. 위의 코드 예제를 자유롭게 실행해 보십시오. 하지만 수렴를 위한 훈련은 상당한 시간이 소요되므로 이 과정을 조기에 끝내고 아래에서 논의하는 코드 마이그레이션에 집중하는 것이 도움이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9d34b-dead-4d67-aee5-2a1c614f8fe6",
   "metadata": {
    "id": "95a9d34b-dead-4d67-aee5-2a1c614f8fe6"
   },
   "source": [
    "## 소개\n",
    "\n",
    "개념적으로 우리의 과제 목표는 다음과 같습니다:\n",
    "- 학습 파이프라인의 독립 실행형 파이토치 구현을 DeepSpeed로 마이그레이션하고 \"2개의 서버\" 클러스터에서 효과적으로 훈련하십시오.\n",
    "- 혼합 정밀도 훈련, 액티베이션 체크 포인팅, 제로 리던던시 옵티마이저와 같은 메모리 절약을 가능하게 하는 기능을 활성화합니다.\n",
    "- 학습 중인 모델의 크기를 늘려봅니다.\n",
    "\n",
    "아래 노트북은 프로세스를 안내하고 테스트 코드를 제공하여 올바른 해결 방법을 모색하는 데 도움이 됩니다. 평가가 끝날 때, 코딩을 완료하면 주피터랩 플랫폼으로 돌아가 'Assess' 버튼을 누릅니다. 버튼을 누르면 코드 파일과 DeepSpeed 구성 파일을 로드하고 이를 실행하여 구현의 정확성을 평가하는 자동화 프로세스가 실행될 예정입니다. 완료하는 데 몇 분 정도 걸릴 수 있으므로 이 단계를 실행할 충분한 시간을 두는 것이 좋습니다. 시간이 촉박한 경우 나중에 완료할 수 있도록 수정한 파일을 다운로드해두는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b959719-1fef-4a5a-8454-5babc9c6aa90",
   "metadata": {
    "id": "9b959719-1fef-4a5a-8454-5babc9c6aa90"
   },
   "source": [
    "## 1단계: 베이스라인 구현\n",
    "\n",
    "평가의 시작 지점인 [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)부터 살펴보겠습니다. 이 파일은 이전에 검토했던 내용과 동일한 코드이며, 배치 실행을 위해 파이썬 파일로 추출되었습니다. 독립 실행형 모드에서 작동하는지 테스트해 보겠습니다. 다시 한 번 말하지만, 수렴을 위한 훈련에는 상당한 시간이 소요되므로, 일단 훈련 진행되는 것이 확인되면 자유롭게 훈련 과정을 중단하고 다음 단계로 넘어갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf98ec2-b1b8-4a6c-b2de-8410ea4562cf",
   "metadata": {
    "id": "acf98ec2-b1b8-4a6c-b2de-8410ea4562cf",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000 10000\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"minGPT/minGPT/runStartingPoint.py\", line 42, in <module>\n",
      "    px = torch.cat([pluck_rgb(x) for x, y in train_data], dim=0).float()\n",
      "  File \"minGPT/minGPT/runStartingPoint.py\", line 42, in <listcomp>\n",
      "    px = torch.cat([pluck_rgb(x) for x, y in train_data], dim=0).float()\n",
      "  File \"minGPT/minGPT/runStartingPoint.py\", line 41, in <lambda>\n",
      "    pluck_rgb = lambda x: torch.from_numpy(np.array(x)).view(32*32, 3)[torch.randperm(32*32)[:5], :]\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/PIL/Image.py\", line 532, in __getattr__\n",
      "    def __getattr__(self, name):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python minGPT/minGPT/runStartingPoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae562f-1b8c-4383-a83d-101983f70ea7",
   "metadata": {
    "id": "19ae562f-1b8c-4383-a83d-101983f70ea7"
   },
   "source": [
    "## 2단계 : DeepSpeed 활성화\n",
    "\n",
    "먼저 코드 변경을 최소화하고 DeepSpeed 라이브러리를 사용하도록 이전 훈련 스크립트를 수정하는 것으로 시작하겠습니다. 이 작업을 위해서는 다음이 필요합니다:\n",
    "\n",
    "&nbsp; &nbsp; 1.  [runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py)  에서 관련되는 부분을 수정합니다.   \n",
    "&nbsp; &nbsp;  2.  [trainer.py](./minGPT/minGPT/mingpt/trainer.py) 에서 관련되는 부분을 수정합니다.   \n",
    "&nbsp; &nbsp;  3.  DeepSpeed 구성 파일 `ds_config_basic.json`   을 생성합니다.   \n",
    "&nbsp; &nbsp;  4. `deepspeed` 명령어를 사용하여 트레이닝을 실행합니다.\n",
    "\n",
    "\n",
    "### 1. `runFirstDeepSpeed.py` 파일의 \"ToDo Step 2\" 섹션을 수정합니다.\n",
    "[runFirstDeepSpeed.py](./minGPT/minGPT/runFirstDeepSpeed.py) 파일을 열고 DeepSpeed에 코드를 이식할 \"ToDo Step 2\" 섹션을 정의합니다. 정의해야 할 섹션은 총 4개입니다.\n",
    "\n",
    "### 2.  `trainer.py` 파일에서 \"ToDo Step 2\" 섹션을 수정합니다.\n",
    "[trainer.py](./minGPT/minGPT/mingpt/trainer.py) 파일을 열고 \"ToDo Step 2\" 섹션을 정의하여 `DeepSpeedTrainer` 클래스를 구현합니다. 수정/구현할 섹션은 총 6개입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba61b3-6626-4ca2-b38c-408dbf57a99f",
   "metadata": {
    "id": "5bba61b3-6626-4ca2-b38c-408dbf57a99f"
   },
   "source": [
    "### 3. DeepSpeed 구성 파일 `ds_config_basic.json`을 생성합니다.\n",
    "다음 셀에서 `FIXME`를 다음과 같이 수정합니다.\n",
    "- GPU당 마이크로 배치 크기는 8입니다.\n",
    "- Adam optimizer를 활성화하고 원래 코드 [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)에서 학습률을 복사해야 합니다.\n",
    "- 그래디언트 클리핑을 원래 코드  [runStartingPoint.py](./minGPT/minGPT/runStartingPoint.py)에 사용된 값으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9eaaaf7-6af1-44d4-8eba-8143e81dcfb3",
   "metadata": {
    "id": "c9eaaaf7-6af1-44d4-8eba-8143e81dcfb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/ds_config_basic.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/ds_config_basic.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 8,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc199bd-2670-4878-be02-f65a9273540a",
   "metadata": {
    "id": "ddc199bd-2670-4878-be02-f65a9273540a"
   },
   "source": [
    "### 4.`deepspeed` 명령어로 트레이닝을 실행합니다.\n",
    "\n",
    "다음 명령어는 4개의 GPU 트레이닝을 진행해야 하며 훈련이 진행되는 것을 확인해야 합니다. 다시 한 번 말하지만, 이 연습의 목표는 이 모델을 수렴하도록 훈련시키는 것이 아닙니다. 훈련이 실행 중이면 실행을 중단하고 다음 단계로 이동할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8ed7e3b8-820b-4757-b0f0-2d6ad801418f",
   "metadata": {
    "id": "8ed7e3b8-820b-4757-b0f0-2d6ad801418f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-02 09:07:40,731] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-12-02 09:07:40,899] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config minGPT/minGPT/ds_config_basic.json\n",
      "[2022-12-02 09:07:42,456] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "[2022-12-02 09:07:42,456] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2022-12-02 09:07:42,456] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2022-12-02 09:07:42,456] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2022-12-02 09:07:42,456] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2022-12-02 09:07:42,456] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2022-12-02 09:07:44,230] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 1/8, re-initialized 4 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 2/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 3/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 4/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 5/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 6/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 7/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "12/02/2022 09:08:21 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "12/02/2022 09:08:21 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "[2022-12-02 09:08:21,237] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "12/02/2022 09:08:21 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "done step 8/8, re-initialized 0 dead clusters\n",
      "12/02/2022 09:08:21 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "12/02/2022 09:08:23 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "12/02/2022 09:08:23 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "12/02/2022 09:08:24 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "12/02/2022 09:08:24 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "12/02/2022 09:08:24 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "12/02/2022 09:08:24 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "12/02/2022 09:08:24 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "12/02/2022 09:08:24 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "[2022-12-02 09:08:24,872] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1764812469482422 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2023310661315918 seconds\n",
      "[2022-12-02 09:08:25,807] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2023942470550537 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.2022264003753662 seconds\n",
      "[2022-12-02 09:08:25,816] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2022-12-02 09:08:25,816] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2022-12-02 09:08:25,816] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2022-12-02 09:08:25,816] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2022-12-02 09:08:25,816] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.999)]\n",
      "[2022-12-02 09:08:25,817] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "[2022-12-02 09:08:25,817] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2022-12-02 09:08:25,817] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2022-12-02 09:08:25,817] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "[2022-12-02 09:08:25,817] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "[2022-12-02 09:08:25,818] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   fp16_enabled ................. False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.0003}\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "[2022-12-02 09:08:25,819] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   train_batch_size ............. 32\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "[2022-12-02 09:08:25,820] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "[2022-12-02 09:08:25,821] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2022-12-02 09:08:25,821] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "[2022-12-02 09:08:25,821] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "[2022-12-02 09:08:25,821] [INFO] [config.py:1065:print]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0003\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0\n",
      "}\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"minGPT/minGPT/runFirstDeepSpeed.py\", line 86, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"minGPT/minGPT/runFirstDeepSpeed.py\", line 86, in <module>\n",
      "  File \"minGPT/minGPT/runFirstDeepSpeed.py\", line 86, in <module>\n",
      "  File \"minGPT/minGPT/runFirstDeepSpeed.py\", line 86, in <module>\n",
      "            trainer.train()trainer.train()trainer.train()\n",
      "\n",
      "\n",
      "  File \"/dli/minGPT/minGPT/mingpt/trainer.py\", line 112, in train\n",
      "  File \"/dli/minGPT/minGPT/mingpt/trainer.py\", line 112, in train\n",
      "  File \"/dli/minGPT/minGPT/mingpt/trainer.py\", line 112, in train\n",
      "    [2022-12-02 09:08:32,709] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 2637\n",
      "trainer.train()\n",
      "  File \"/dli/minGPT/minGPT/mingpt/trainer.py\", line 112, in train\n",
      "        model_engine, optimizer, train_loader, _ = deepspeed.initialize(args=self.config.cmd_args,model_engine, optimizer, train_loader, _ = deepspeed.initialize(args=self.config.cmd_args,\n",
      "\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/__init__.py\", line 120, in initialize\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/__init__.py\", line 120, in initialize\n",
      "    model_engine, optimizer, train_loader, _ = deepspeed.initialize(args=self.config.cmd_args,\n",
      "      File \"/opt/conda/lib/python3.8/site-packages/deepspeed/__init__.py\", line 120, in initialize\n",
      "engine = DeepSpeedEngine(args=args,    \n",
      "engine = DeepSpeedEngine(args=args,  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 342, in __init__\n",
      "\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 342, in __init__\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 342, in __init__\n",
      "[2022-12-02 09:08:32,710] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 2638\n",
      "    util_ops = UtilsBuilder().load()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py\", line 463, in load\n",
      "    util_ops = UtilsBuilder().load()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py\", line 463, in load\n",
      "    [2022-12-02 09:08:32,710] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 2639\n",
      "return self.jit_load(verbose)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py\", line 505, in jit_load\n",
      "    return self.jit_load(verbose)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py\", line 505, in jit_load\n",
      "[2022-12-02 09:08:32,711] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 2640\n",
      "    op_module = load([2022-12-02 09:08:32,711] [INFO] [launch.py:187:sigkill_handler] Main process received SIGINT, exiting\n"
     ]
    }
   ],
   "source": [
    "!deepspeed minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config minGPT/minGPT/ds_config_basic.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ed01e-9ca5-441c-9275-d4573115d1b7",
   "metadata": {
    "id": "583ed01e-9ca5-441c-9275-d4573115d1b7"
   },
   "source": [
    "## 3단계: 멀티 노드 실행\n",
    "\n",
    "위 코드는 이 특정 노드에 대해 4개의 GPU에서 실행되었지만, 우리의 목표는 수업 초반에 사용한 두개의 노드에서 작동하도록 만드는 것입니다. 앞서 작업한 코드를 다시 사용하여 위의 두개의 노드에서 실행되도록 작업 코드를 수정하세요. 적절한 Shell 스크립트를 생성하는 것부터 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "62ab689f-eb64-4881-a7da-fbf963bd7721",
   "metadata": {
    "id": "62ab689f-eb64-4881-a7da-fbf963bd7721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep3.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep3.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step3\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runFirstDeepSpeed.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config minGPT/minGPT/ds_config_basic.json #FIXEME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83193290-4079-4a9d-a1b0-17f939b45d29",
   "metadata": {
    "id": "83193290-4079-4a9d-a1b0-17f939b45d29"
   },
   "source": [
    "멀티 노드 실행을 활성화하려면 아래를 수정하십시오. 아래 명령어를 사용하여 멀티 노드 작업을 실행하십시오 (평가 제출 시 사용할 명령어이므로 파일 이름이나 경로를 변경하지 마십시오)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7fae3988-8796-4c1c-873a-f4828c90310c",
   "metadata": {
    "id": "7fae3988-8796-4c1c-873a-f4828c90310c",
    "outputId": "990eb47c-3f89-42b5-dbeb-b54e9745e804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 12\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                12  slurmpar dli_asse    admin  R       0:01      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!sbatch ./minGPT/minGPT/runSlurmStep3.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3d948-32ac-4043-aa99-1b0ce50108a8",
   "metadata": {
    "id": "a3d3d948-32ac-4043-aa99-1b0ce50108a8"
   },
   "source": [
    "위 명령어가 실행되면 아래 명령어로 출력 및 오류 로그를 볼 수 있습니다. 작업 ID를 아래 명령어로 복사하십시오. 다시 한 번 코드 배포 시 아래 파일 위치에 다음과 같은 파일명 구조로 로그를 남기는 지 확인하십시오. 동일하게 평가 제출 시에도 해당 로그 파일들에 대한 확인이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e584a73-c60d-43c0-bd6c-6c2a4725ae83",
   "metadata": {
    "id": "6e584a73-c60d-43c0-bd6c-6c2a4725ae83",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-02 09:08:50,536] [INFO] [runner.py:378:main] Using IP address of 172.18.0.9 for node slurmnode1\n",
      "[2022-12-02 09:08:50,537] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2022-12-02 09:08:50,537] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.9 --master_port=29500 /dli/minGPT/minGPT/runFirstDeepSpeed.py --deepspeed --deepspeed_config 'minGPT/minGPT/ds_config_basic.json'\n",
      "slurmnode1: [2022-12-02 09:08:52,844] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2022-12-02 09:08:52,844] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2022-12-02 09:08:52,844] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2022-12-02 09:08:52,844] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2022-12-02 09:08:52,844] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2022-12-02 09:08:52,845] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode2: [2022-12-02 09:08:52,874] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2022-12-02 09:08:52,874] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2022-12-02 09:08:52,874] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2022-12-02 09:08:52,874] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2022-12-02 09:08:52,874] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2022-12-02 09:08:52,874] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2022-12-02 09:08:54,583] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode1: \n",
      "slurmnode2: Files already downloaded and verifiedFiles already downloaded and verified\n",
      "slurmnode2: \n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode2: done step 1/8, re-initialized 4 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 2/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 3/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 4/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 5/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 6/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 7/8, re-initialized 0 dead clusters\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2022-12-02 09:09:30,797] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "slurmnode2: done step 8/8, re-initialized 0 dead clusters\n",
      "slurmnode1: [2022-12-02 09:09:34,188] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Detected CUDA files, patching ldflags\n",
      "slurmnode2: Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "slurmnode2: Building extension module fused_adam...\n",
      "slurmnode2: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: ninja: no work to do.\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.08805012702941895 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode2: Loading extension module fused_adam...\n",
      "slurmnode2: Time to load fused_adam op: 0.10219478607177734 seconds\n",
      "slurmnode2: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.10221624374389648 seconds\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "slurmnode1: Loading extension module fused_adam...\n",
      "slurmnode1: Time to load fused_adam op: 0.1019906997680664 seconds\n",
      "slurmnode1: [2022-12-02 09:09:35,046] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "slurmnode1: [2022-12-02 09:09:35,055] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "slurmnode1: [2022-12-02 09:09:35,055] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "slurmnode1: [2022-12-02 09:09:35,055] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "slurmnode1: [2022-12-02 09:09:35,055] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "slurmnode1: [2022-12-02 09:09:35,055] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003], mom=[(0.9, 0.999)]\n",
      "slurmnode1: [2022-12-02 09:09:35,056] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "slurmnode1: [2022-12-02 09:09:35,056] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "slurmnode1:     \"partition_activations\": false, \n",
      "slurmnode1:     \"contiguous_memory_optimization\": false, \n",
      "slurmnode1:     \"cpu_checkpointing\": false, \n",
      "slurmnode1:     \"number_checkpoints\": null, \n",
      "slurmnode1:     \"synchronize_checkpoint_boundary\": false, \n",
      "slurmnode1:     \"profile\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2022-12-02 09:09:35,056] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "slurmnode1: [2022-12-02 09:09:35,056] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"start_step\": null, \n",
      "slurmnode1:     \"end_step\": null, \n",
      "slurmnode1:     \"metric_path\": null, \n",
      "slurmnode1:     \"arg_mappings\": null, \n",
      "slurmnode1:     \"metric\": \"throughput\", \n",
      "slurmnode1:     \"model_info\": null, \n",
      "slurmnode1:     \"results_dir\": null, \n",
      "slurmnode1:     \"exps_dir\": null, \n",
      "slurmnode1:     \"overwrite\": true, \n",
      "slurmnode1:     \"fast\": true, \n",
      "slurmnode1:     \"start_profile_step\": 3, \n",
      "slurmnode1:     \"end_profile_step\": 5, \n",
      "slurmnode1:     \"tuner_type\": \"gridsearch\", \n",
      "slurmnode1:     \"tuner_early_stopping\": 5, \n",
      "slurmnode1:     \"tuner_num_trials\": 50, \n",
      "slurmnode1:     \"model_info_path\": null, \n",
      "slurmnode1:     \"mp_size\": 1, \n",
      "slurmnode1:     \"max_train_batch_size\": null, \n",
      "slurmnode1:     \"min_train_batch_size\": 1, \n",
      "slurmnode1:     \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "slurmnode1:     \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "slurmnode1:     \"num_tuning_micro_batch_sizes\": 3\n",
      "slurmnode1: }\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... None\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "slurmnode1: [2022-12-02 09:09:35,057] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "slurmnode1:     \"enabled\": false, \n",
      "slurmnode1:     \"profile_step\": 1, \n",
      "slurmnode1:     \"module_depth\": -1, \n",
      "slurmnode1:     \"top_modules\": 1, \n",
      "slurmnode1:     \"detailed\": true, \n",
      "slurmnode1:     \"output_file\": null\n",
      "slurmnode1: }\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   fp16_enabled ................. False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.0003}\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
      "slurmnode1: [2022-12-02 09:09:35,058] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   train_batch_size ............. 32\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "slurmnode1:     \"stage\": 0, \n",
      "slurmnode1:     \"contiguous_gradients\": true, \n",
      "slurmnode1:     \"reduce_scatter\": true, \n",
      "slurmnode1:     \"reduce_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"allgather_partitions\": true, \n",
      "slurmnode1:     \"allgather_bucket_size\": 5.000000e+08, \n",
      "slurmnode1:     \"overlap_comm\": false, \n",
      "slurmnode1:     \"load_from_fp32_weights\": true, \n",
      "slurmnode1:     \"elastic_checkpoint\": false, \n",
      "slurmnode1:     \"offload_param\": null, \n",
      "slurmnode1:     \"offload_optimizer\": null, \n",
      "slurmnode1:     \"sub_group_size\": 1.000000e+09, \n",
      "slurmnode1:     \"prefetch_bucket_size\": 5.000000e+07, \n",
      "slurmnode1:     \"param_persistence_threshold\": 1.000000e+05, \n",
      "slurmnode1:     \"max_live_parameters\": 1.000000e+09, \n",
      "slurmnode1:     \"max_reuse_distance\": 1.000000e+09, \n",
      "slurmnode1:     \"gather_16bit_weights_on_model_save\": false, \n",
      "slurmnode1:     \"ignore_unused_parameters\": true, \n",
      "slurmnode1:     \"round_robin_gradients\": false, \n",
      "slurmnode1:     \"legacy_stage1\": false\n",
      "slurmnode1: }\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "slurmnode1: [2022-12-02 09:09:35,059] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "slurmnode1: [2022-12-02 09:09:35,061] [INFO] [config.py:1065:print]   json = {\n",
      "slurmnode1:     \"train_micro_batch_size_per_gpu\": 8, \n",
      "slurmnode1:     \"optimizer\": {\n",
      "slurmnode1:         \"type\": \"Adam\", \n",
      "slurmnode1:         \"params\": {\n",
      "slurmnode1:             \"lr\": 0.0003\n",
      "slurmnode1:         }\n",
      "slurmnode1:     }, \n",
      "slurmnode1:     \"gradient_clipping\": 1.0\n",
      "slurmnode1: }\n",
      "slurmnode1: Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=12;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "83fe2de0-5d66-4880-ab10-074bc896e1ae",
   "metadata": {
    "id": "83fe2de0-5d66-4880-ab10-074bc896e1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1: 12/02/2022 09:09:29 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 12/02/2022 09:09:30 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 12/02/2022 09:09:30 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode2: 12/02/2022 09:09:31 - INFO - mingpt.model -   number of parameters: 1.000166e+07\n",
      "slurmnode1: 12/02/2022 09:09:32 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "slurmnode1: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "slurmnode2: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "slurmnode2: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "slurmnode2: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode1: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n",
      "slurmnode2: 12/02/2022 09:09:33 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=12;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cbe44-51ad-4411-8ef5-792075915855",
   "metadata": {
    "id": "ee6cbe44-51ad-4411-8ef5-792075915855"
   },
   "source": [
    "코드가 만족스러우면 다음 단계로 이동하기 전에 배치 작업이 종료되었는지 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3c2e59b2-a040-40e7-bb47-022eba1f15d3",
   "metadata": {
    "id": "3c2e59b2-a040-40e7-bb47-022eba1f15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                12  slurmpar dli_asse    admin  R       1:12      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a98f6482-d538-4e28-b299-5bc0e3824cc3",
   "metadata": {
    "id": "a98f6482-d538-4e28-b299-5bc0e3824cc3"
   },
   "outputs": [],
   "source": [
    "!scancel 12 #PASTE_JOB_ID_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18320d-6f90-41b5-93ea-b2c6afc36317",
   "metadata": {
    "id": "1c18320d-6f90-41b5-93ea-b2c6afc36317"
   },
   "source": [
    "## 4단계: 코드를 추가로 개선하기\n",
    "\n",
    "현재 우리는 활성화 체크포인팅을 수행할 수 있는 기능이 없습니다. 이번 단계에서는 DeepSpeed 라이브러리로 활성화 체크포인팅을 실행할 수 있는 코드를 소개합니다.\n",
    "\n",
    "&nbsp; &nbsp;  1. 활성화 체크포인팅을 위한 트랜스포머 블록을 정의합니다.   \n",
    "&nbsp; &nbsp;  2. 활성화 체크포인팅 및 FP16 트레이닝을 지원하는 DeepSeed 구성 파일을 만듭니다.   \n",
    "&nbsp; &nbsp;  3.  sbatch 트레이닝 파일을 만들고 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1cd8f",
   "metadata": {
    "id": "b7c1cd8f"
   },
   "source": [
    "### 1. 활성화 체크포인팅을 위한 트랜스포머 블록을 정의합니다.\n",
    "\n",
    "DeepSpeed를 사용하여 모델(또는 모델의 일부)의 활성화 체크포인팅을 활성화하려면 포워드 패스 정의에서 각 블록을 `deepspeed.checkpointing.checkpoint()` ([더 알아보기](https://deepspeed.readthedocs.io/en/stable/activation-checkpointing.html#deepspeed.checkpointing.checkpoint)) 함수로 감싸야 합니다.\n",
    "\n",
    "아래 예제는 CNN 블록 2개와 DeepSpeed로 활성화 체크포인팅을 위해CNN 블록을 감싸는 선형 레이어가 있는 간단한 합성곱 신경망 정의를 보여줍니다.\n",
    "\n",
    "```\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_block_1 = nn.Sequential(*[nn.Conv2d(3, 32, 3, padding=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2)])\n",
    "        self.cnn_block_2 = nn.Sequential(*[nn.Conv2d(64, 64, 3, padding=1),nn.ReLU(),nn.MaxPool2d(kernel_size=2)])\n",
    "        self.flatten = lambda inp: torch.flatten(inp, 1)\n",
    "        self.linearize = nn.Sequential(*[ nn.Linear(64 * 8 * 8, 512),nn.ReLU()])\n",
    "        self.out = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = deepspeed.checkpointing.checkpoint(self.cnn_block_1, X)\n",
    "        X = deepspeed.checkpointing.checkpoint(self.cnn_block_2, X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.linearize(X)\n",
    "        X = self.out(X)\n",
    "        return X\n",
    "\n",
    "```\n",
    "\n",
    "비슷한 메커니즘은 `torch.utils.checkpoint.checkpoint()`기능을 통해 토치와 함께 구현됩니다.\n",
    "\n",
    "\n",
    "우리의 경우, Vision Transformer 모델은 `./minGPT/minGPT/mingpt/model.py` 파일의 GPT 클래스로 구현됩니다. DeepSpeed 활성화 체크포인팅으로 트랜스포머 블록을 감싸야 합니다.  [model.py](./minGPT/minGPT/mingpt/model.py 파일에서 \"Step 4 ToDo\" 태스크를 수정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533ffd5",
   "metadata": {
    "id": "3533ffd5"
   },
   "source": [
    "### 2. DeepSeed 구성 파일을 만듭니다.\n",
    "\n",
    "시작하기 전에 config-json 파일의 DeepSpeed 문서에서 [activation-checkpointing](https://www.deepspeed.ai/docs/config-json/#activation-checkpointing) 을 확인해볼 수 있습니다.\n",
    "\n",
    "아래 셀의 `#FIXME`를 다음과 같이 수정하여 `ds_config_step4.json` 을 만듭니다.\n",
    "- 활성화 체크포인팅을 활성화합니다.\n",
    "- 활성화 체크포인팅이 제대로 작동하는지 확인하려면 GPU당 마이크로 배치 크기를 128로 설정하십시오.\n",
    "- 체크포인트 수를 12개로 만듭니다.\n",
    "- FP16 트레이닝을 활성화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0112af26",
   "metadata": {
    "id": "0112af26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minGPT/minGPT/ds_config_step4.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile minGPT/minGPT/ds_config_step4.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 128,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": true,\n",
    "        \"number_checkpoints\": 12,\n",
    "        \"cpu_checkpointing\": true\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": true,\n",
    "        \"auto_cast\": false\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f43cef",
   "metadata": {
    "id": "72f43cef"
   },
   "source": [
    "### 3. sbatch 트레이닝 파일을 실행합니다.\n",
    "\n",
    "\n",
    "먼저 훈련용 파이썬 스크립트`runFirstDeepSpeed.py`의 복사본을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3bad5d",
   "metadata": {
    "id": "aa3bad5d"
   },
   "outputs": [],
   "source": [
    "!cp /dli/minGPT/minGPT/runFirstDeepSpeed.py /dli/minGPT/minGPT/runStep4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37d3a0",
   "metadata": {
    "id": "af37d3a0"
   },
   "source": [
    "이제 sbatch 파일 `runSlurmStep4.sh`을 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7efe061f",
   "metadata": {
    "id": "7efe061f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep4.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep4.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step4\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runStep4.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/minGPT/minGPT/ds_config_step4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb16851",
   "metadata": {
    "id": "0cb16851"
   },
   "source": [
    "위의 작업을 완료한 후 아래 명령어를 실행하여 훈련 작업을 슬럼 스케줄러에 제출하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a10d38-9eda-41e2-bcce-8156caced370",
   "metadata": {
    "id": "e8a10d38-9eda-41e2-bcce-8156caced370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 15\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                15  slurmpar dli_asse    admin PD       0:00      2 (None)\n",
      "                14  slurmpar dli_asse    admin  R       1:11      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!sbatch /dli/minGPT/minGPT/runSlurmStep4.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7261f-bd99-4cea-98b6-5be237aa3446",
   "metadata": {
    "id": "59e7261f-bd99-4cea-98b6-5be237aa3446"
   },
   "source": [
    "아래를 사용하여 코드 실행을 확인합니다 (배치 크기가 크더라도 진행이 확인되어야 합니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3bf8a5-c6f8-4e16-89ea-b65cfc0aefe1",
   "metadata": {
    "id": "0e3bf8a5-c6f8-4e16-89ea-b65cfc0aefe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-02 09:14:05,376] [INFO] [runner.py:378:main] Using IP address of 172.18.0.9 for node slurmnode1\n",
      "[2022-12-02 09:14:05,377] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2022-12-02 09:14:05,377] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.9 --master_port=29500 /dli/minGPT/minGPT/runStep4.py --deepspeed --deepspeed_config '/dli/minGPT/minGPT/ds_config_step4.json'\n",
      "slurmnode2: [2022-12-02 09:14:07,690] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2022-12-02 09:14:07,690] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2022-12-02 09:14:07,690] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2022-12-02 09:14:07,691] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2022-12-02 09:14:07,691] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2022-12-02 09:14:07,691] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2022-12-02 09:14:07,724] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2022-12-02 09:14:07,724] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2022-12-02 09:14:07,724] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2022-12-02 09:14:07,725] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2022-12-02 09:14:07,725] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2022-12-02 09:14:07,725] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2022-12-02 09:14:09,480] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: [2022-12-02 09:14:09,740] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 3696\n",
      "slurmnode1: [2022-12-02 09:14:09,740] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 3697\n",
      "slurmnode1: [2022-12-02 09:14:09,741] [ERROR] [launch.py:184:sigkill_handler] ['/opt/conda/bin/python3.8', '-u', '/dli/minGPT/minGPT/runStep4.py', '--local_rank=1', '--deepspeed', '--deepspeed_config', '/dli/minGPT/minGPT/ds_config_step4.json'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=15;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f84c360-9eb9-41aa-ab6c-b1eaec2add62",
   "metadata": {
    "id": "7f84c360-9eb9-41aa-ab6c-b1eaec2add62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1: Traceback (most recent call last):\n",
      "slurmnode1:   File \"/dli/minGPT/minGPT/runStep4.py\", line 36, in <module>\n",
      "slurmnode1:     deepspeed.init_distributed()\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/deepspeed/utils/distributed.py\", line 51, in init_distributed\n",
      "slurmnode1:     torch.distributed.init_process_group(backend=dist_backend,\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 578, in init_process_group\n",
      "slurmnode1:     store, rank, world_size = next(rendezvous_iterator)\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "slurmnode1:     store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "slurmnode1:     return TCPStore(\n",
      "slurmnode1: RuntimeError: Address already in use\n",
      "pdsh@slurmnode1: slurmnode1: ssh exited with exit code 1\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=15;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e2925-d6f1-456b-bfa9-dbec7325df58",
   "metadata": {
    "id": "817e2925-d6f1-456b-bfa9-dbec7325df58"
   },
   "source": [
    "코드 실행이 만족스럽다면 다음 단계 이동을 위해 배치 작업의 실행을 취소하는 것을 잊지 마십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cefa60c4-e58f-435e-9960-64a8445308e9",
   "metadata": {
    "id": "cefa60c4-e58f-435e-9960-64a8445308e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                15  slurmpar dli_asse    admin  R       0:03      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5904dcb-3a59-43c4-8806-d5320324855b",
   "metadata": {
    "id": "a5904dcb-3a59-43c4-8806-d5320324855b"
   },
   "outputs": [],
   "source": [
    "!scancel 14 #PASTE_JOB_ID_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112d963",
   "metadata": {
    "id": "f112d963"
   },
   "source": [
    "### 추가적인 최적화 고려 사항\n",
    "\n",
    "훈련 과정에 참여하는 모든 워커는 동일한 출력을 생성합니다. 따라서 k-mean 값은 두 번 계산됩니다. \n",
    "k-mean 구현을 조정하여 한 번만 실행하고 결과를 모든 워커에 재배포할 수 있습니다. \n",
    "다음은 어떻게 실행할 수 있는 지에 대한 예시입니다.\n",
    "\n",
    "```import torch.distributed as dist\n",
    "def run_kmeans(x, ncluster, niter=8, rank, size):\n",
    "    print('KMeans executed on rank ', rank, ' Worlds size ', size)\n",
    "    N, D = x.size()\n",
    "    c = x[torch.randperm(N)[:ncluster]] # init clusters at random\n",
    "    c = c.cuda(args.local_rank) # move the tensor to the GPU for exchange\n",
    "    if rank == 0:\n",
    "        # Computing KMeans only on rank 0 \n",
    "        with torch.no_grad():\n",
    "            c = kmeans(x, ncluster, niter)\n",
    "    # We now have computed the clusters so can proceed to the exchange\n",
    "    dist.barrier()\n",
    "    print('Broadcasting')\n",
    "    dist.broadcast(C.cuda(args.local_rank), src=0)\n",
    "    c=c.cpu()\n",
    "    print('Rank ', rank, ' has data ', C.size())\n",
    "    return c\n",
    "\n",
    "C=run_kmeans(px, ncluster, niter=8, dist.get_rank(), dist.get_world_size())    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2bd1d-d375-4db3-815f-9ae64336805f",
   "metadata": {
    "id": "42e2bd1d-d375-4db3-815f-9ae64336805f"
   },
   "source": [
    "## 5단계: 스케일업\n",
    "\n",
    "이제 최소한의 기능이 구현되었으므로 훈련 작업을 확장해 보겠습니다. 평가에서 스케일업 부분은 모델을 상당히 크게 만들 것입니다. \n",
    "\n",
    "&nbsp; &nbsp; 1. 모델의 아키텍처를 확장합니다.   \n",
    "&nbsp; &nbsp; 2. 활성화 체크포인팅, FP16 트레이닝, ZeRO 옵티마이저를 활성화하는 DeepSeed 구성 파일을 만듭니다.     \n",
    "&nbsp; &nbsp; 3. sbatch 훈련 파일을 만들고 실행합니다.  \n",
    "\n",
    "### 1. 모델의 아키텍처를 확장합니다.\n",
    "트레이닝 스크립트를 수정하기 전에 수정할 복사본을 만드는 것부터 시작하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5161b46d-16d2-4666-ba8a-a687d1912f93",
   "metadata": {
    "id": "5161b46d-16d2-4666-ba8a-a687d1912f93"
   },
   "outputs": [],
   "source": [
    "!cp /dli/minGPT/minGPT/runFirstDeepSpeed.py /dli/minGPT/minGPT/runStep5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d6341-5b42-4d1f-a6cd-365e0bb4fbbf",
   "metadata": {
    "id": "2e2d6341-5b42-4d1f-a6cd-365e0bb4fbbf"
   },
   "source": [
    " [runStep5.py](./minGPT/minGPT/runStep5.py)에서  \"GPTConfig\" 섹션을 수정하여 VisionTransformers의 레이어 수를 **24** 로 조정합니다.  \"GPTConfig\" 에서는 신경망 차원의 아키텍처가 다음과 같이 정의됩니다.\n",
    "\n",
    "```\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  embd_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0,\n",
    "                  n_layer=12, n_head=8, n_embd=256)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. 활성화 체크포인팅, FP16 트레이닝, ZeRO 옵티마이저를 지원하는 DeepSeed 구성 파일을 만듭니다.\n",
    "\n",
    "[ds_config_step5.json](./minGPT/minGPT/ds_config_step5.json) 을 변경하여 다음을 사용하도록 재구성합니다.:\n",
    "- 그래디언트 누적 및 4단계 누적 단계를 실행하여 글로벌 배치 크기를 늘립니다 (고정된 하이퍼 파라미터를 유지하는 데 자주 필요합니다).\n",
    "- 12개의 체크포인트가 아닌 24개의 체크포인트를 만들기 위한 활성화 체크포인팅을 실행합니다.\n",
    "- FP16 트레이닝\n",
    "- 파라미터 및 옵티마이저 상태 모두에 대해 CPU 오프로드를 지원하는 Zero Stage 3 옵티마이저를 실행합니다. 자세한 내용은  [ZeRO 설명서](https://deepspeed.readthedocs.io/en/latest/zero3.html) 를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ce4ee5-52a3-4611-9438-242b1b2837b2",
   "metadata": {
    "id": "64ce4ee5-52a3-4611-9438-242b1b2837b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minGPT/minGPT/ds_config_step5.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile minGPT/minGPT/ds_config_step5.json\n",
    "{\n",
    "  \"train_micro_batch_size_per_gpu\": 128,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 3e-4\n",
    "    }\n",
    "  },\n",
    "  \"gradient_clipping\": 1.0,\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": true,\n",
    "        \"number_checkpoints\": 24,\n",
    "        \"cpu_checkpointing\": true\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": true,\n",
    "        \"auto_cast\": false\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "    \"offload_optimizer\": {\n",
    "        \"device\": \"cpu\"\n",
    "    },\n",
    "    \"offload_params\": {\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "    }\n",
    "}\n",
    "\n",
    " # FIXME enable activation_checkpointing  (v)\n",
    " # FIXME enable FP16 training (v)\n",
    " # FIXME enable zero_optimization stage 3 with CPU offload for both parameters and optimizer states  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338d1e4-5814-48b6-935f-10a111f8f76e",
   "metadata": {
    "id": "4338d1e4-5814-48b6-935f-10a111f8f76e"
   },
   "source": [
    "### 3. sbatch 트레이닝 파일을 만들고 실행합니다. \n",
    "다음 셀을 실행하여 5단계 훈련에 대한 sbatch 스크립트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b9051e-1386-4aff-be66-d16d637f9377",
   "metadata": {
    "id": "c5b9051e-1386-4aff-be66-d16d637f9377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./minGPT/minGPT/runSlurmStep5.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./minGPT/minGPT/runSlurmStep5.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_assessment_step5\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "# Number of nodes\n",
    "NUM_NODES=2\n",
    "# Number of GPUs per node\n",
    "NUM_GPUS=2\n",
    "\n",
    "deepspeed --num_nodes=${NUM_NODES} --hostfile /dli/minGPT/minGPT/hostfile --num_gpus=${NUM_GPUS} /dli/minGPT/minGPT/runStep5.py \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/minGPT/minGPT/ds_config_step5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a41a70-bc32-439c-a9f2-8a9faf70fe6f",
   "metadata": {
    "id": "c9a41a70-bc32-439c-a9f2-8a9faf70fe6f"
   },
   "source": [
    "위의 내용을 변경한 후에는 다음 명령어를 사용하여 작업을 실행하십시오:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40801a02-10c5-4dd6-be57-ed2a5ad24516",
   "metadata": {
    "id": "40801a02-10c5-4dd6-be57-ed2a5ad24516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                16  slurmpar dli_asse    admin PD       0:00      2 (Resources)\n",
      "                15  slurmpar dli_asse    admin  R       0:30      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!sbatch /dli/minGPT/minGPT/runSlurmStep5.sh\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef1ba9-40e3-421d-b3d4-3bf9cb1350c9",
   "metadata": {
    "id": "2fef1ba9-40e3-421d-b3d4-3bf9cb1350c9"
   },
   "source": [
    "다음을 사용하여 코드가 실행되었는지 확인합니다:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb23a05d-cfe3-4049-b97b-c8370ed38799",
   "metadata": {
    "id": "cb23a05d-cfe3-4049-b97b-c8370ed38799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-02 09:14:44,540] [INFO] [runner.py:378:main] Using IP address of 172.18.0.9 for node slurmnode1\n",
      "[2022-12-02 09:14:44,541] [INFO] [multinode_runner.py:65:get_cmd] Running on the following workers: slurmnode1,slurmnode2\n",
      "[2022-12-02 09:14:44,541] [INFO] [runner.py:457:main] cmd = pdsh -f 1024 -w slurmnode1,slurmnode2 export NCCL_VERSION=2.11.4; export PYTHONPATH=/dli:/etc/assessment/; export PYTHONIOENCODING=utf-8;  cd /dli; /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJzbHVybW5vZGUxIjogWzAsIDFdLCAic2x1cm1ub2RlMiI6IFswLCAxXX0= --node_rank=%n --master_addr=172.18.0.9 --master_port=29500 /dli/minGPT/minGPT/runStep5.py --deepspeed --deepspeed_config '/dli/minGPT/minGPT/ds_config_step5.json'\n",
      "slurmnode1: [2022-12-02 09:14:46,831] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "slurmnode1: [2022-12-02 09:14:46,831] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode1: [2022-12-02 09:14:46,831] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=0\n",
      "slurmnode1: [2022-12-02 09:14:46,831] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode1: [2022-12-02 09:14:46,831] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode1: [2022-12-02 09:14:46,832] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode2: [2022-12-02 09:14:46,897] [INFO] [launch.py:96:main] 1 NCCL_VERSION=2.11.4\n",
      "slurmnode2: [2022-12-02 09:14:46,898] [INFO] [launch.py:103:main] WORLD INFO DICT: {'slurmnode1': [0, 1], 'slurmnode2': [0, 1]}\n",
      "slurmnode2: [2022-12-02 09:14:46,898] [INFO] [launch.py:109:main] nnodes=2, num_local_procs=2, node_rank=1\n",
      "slurmnode2: [2022-12-02 09:14:46,898] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'slurmnode1': [0, 1], 'slurmnode2': [2, 3]})\n",
      "slurmnode2: [2022-12-02 09:14:46,899] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "slurmnode2: [2022-12-02 09:14:46,899] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "slurmnode1: [2022-12-02 09:14:48,579] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "slurmnode1: [2022-12-02 09:14:48,847] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 3931\n",
      "slurmnode1: [2022-12-02 09:14:48,847] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 3932\n",
      "slurmnode1: [2022-12-02 09:14:48,848] [ERROR] [launch.py:184:sigkill_handler] ['/opt/conda/bin/python3.8', '-u', '/dli/minGPT/minGPT/runStep5.py', '--local_rank=1', '--deepspeed', '--deepspeed_config', '/dli/minGPT/minGPT/ds_config_step5.json'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=16;cat /dli/megatron/logs/$JOB_ID.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c41933e8-7789-495a-8b97-3be96ff8f99a",
   "metadata": {
    "id": "c41933e8-7789-495a-8b97-3be96ff8f99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1: Traceback (most recent call last):\n",
      "slurmnode1:   File \"/dli/minGPT/minGPT/runStep5.py\", line 36, in <module>\n",
      "slurmnode1:     deepspeed.init_distributed()\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/deepspeed/utils/distributed.py\", line 51, in init_distributed\n",
      "slurmnode1:     torch.distributed.init_process_group(backend=dist_backend,\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 578, in init_process_group\n",
      "slurmnode1:     store, rank, world_size = next(rendezvous_iterator)\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n",
      "slurmnode1:     store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n",
      "slurmnode1:   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 157, in _create_c10d_store\n",
      "slurmnode1:     return TCPStore(\n",
      "slurmnode1: RuntimeError: Address already in use\n",
      "pdsh@slurmnode1: slurmnode1: ssh exited with exit code 1\n"
     ]
    }
   ],
   "source": [
    "!JOB_ID=16;cat /dli/megatron/logs/$JOB_ID.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae45e4-bef1-4394-b448-5162d9d4b8cb",
   "metadata": {
    "id": "1bae45e4-bef1-4394-b448-5162d9d4b8cb"
   },
   "source": [
    "다음 단계로 이동하기 전에 실행 및 보류 중인 모든 작업을 중지해야합니다. 아니면 평가 결과 Fail을 받을 것입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b15fb509-ab9e-4d39-95cd-28351033d830",
   "metadata": {
    "id": "b15fb509-ab9e-4d39-95cd-28351033d830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                16  slurmpar dli_asse    admin  R       0:39      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d58077c-c4d4-46fa-b4c4-1d9442eb14c9",
   "metadata": {
    "id": "3d58077c-c4d4-46fa-b4c4-1d9442eb14c9"
   },
   "outputs": [],
   "source": [
    "!scancel 15 #PASTE_JOB_ID_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4db940-8ab3-4282-91c6-d631dfbf571a",
   "metadata": {
    "id": "0a4db940-8ab3-4282-91c6-d631dfbf571a"
   },
   "source": [
    "## 6단계: 평가\n",
    "\n",
    "위에 나열된 변경 사항을 모두 구현한 경우 5단계에서 확인한 작업 ID를 아래 코드 블록에 입력하십시오. 문제가 올바르게 완료된 경우 \"Assessment Passed!\" 메시지가 나타납니다. 행운을 빌겠습니다! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b806b2b0-a877-4c02-a88b-e2cb944be111",
   "metadata": {
    "id": "b806b2b0-a877-4c02-a88b-e2cb944be111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Job ID: 5\n",
      "Error: job did not complete a training step with the expected configuration. Please try again.\n"
     ]
    }
   ],
   "source": [
    "from run_assessment import run_assessment\n",
    "job_id = 5\n",
    "run_assessment(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c537006-d407-44b1-8741-1531bfc8609b",
   "metadata": {
    "id": "0c537006-d407-44b1-8741-1531bfc8609b"
   },
   "source": [
    "\"Assessment Passed!\"가 나타나면 DLI 포털로 돌아가서 평가(assess) 버튼을 누르시면 인증서가 생성됩니다. 성공적으로 과정을 마무리하셨습니다! 축하합니다!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('taas_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6646e4aa2c2731f4d272db376bfa5d327ed8a66b77a1099c7f9acfe548f871da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
